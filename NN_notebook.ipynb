{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n",
      "Using Theano backend.\n",
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import keras\n",
    "from keras.utils import np_utils \n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.regularizers import l1, l2,l1l2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "\t'Class for data object'\n",
    "\t\n",
    "\tdef __init__(self,trainFile,trainLabelFile,valRatio):\n",
    "\t\tself.trainFile = trainFile;\n",
    "\t\tself.trainLabelFile = trainLabelFile;\n",
    "# \t\tself.testFile = testFile;\n",
    "\t\tself.valRatio = valRatio;\n",
    "\t\t\n",
    "\tdef inputPreprocess(self):\n",
    "\t\ttrainData = pd.read_csv(self.trainFile);\n",
    "\t\ttrainLabel = pd.read_table(self.trainLabelFile,header = None);\n",
    "\t\t# The number of input features\n",
    "\t\t\n",
    "\t\ttrainData = trainData.replace(-1,np.nan);\n",
    "\t\ttrainData = trainData.fillna(trainData.mean())\n",
    "\t\tprint trainData.mean()\n",
    "\t\tself.features = trainData.shape[1];\n",
    "\t\tprint self.features\n",
    "\t\ttrainData = trainData.as_matrix();\n",
    "\t\ttrainLabel = trainLabel.as_matrix();\n",
    "\t\t# [self.trainInputs, self.valInputs, self.trainTargets, self.valTargets] = train_test_split(trainData,trainLabel ,test_size = self.valRatio)\t\n",
    "# \t\t\n",
    "\n",
    "\t\tself.trainInputs = trainData;\n",
    "\t\tself.trainTargets = trainLabel;\n",
    "\t\tprint self.trainInputs\n",
    "\t\tself.trainMean = np.mean(self.trainInputs,axis=0)\n",
    "\t\ttnStd = np.std(self.trainInputs,axis=0,dtype = np.float32)\n",
    "\t\tself.trainStd = np.array(list(tnStd))\n",
    "\t\tself.trainInputs = z_score_inputs(self.trainInputs,self.trainMean,self.trainStd);\n",
    "# \t\tself.valInputs = z_score_inputs(self.valInputs,self.trainMean,self.trainStd);\n",
    "\t\tprint self.trainInputs\n",
    "\t\t\n",
    "\t\t\n",
    "def z_score_inputs(Inputs, Mean, StdDev):\n",
    "\t\"\"\" \n",
    "\t\tPre-process inputs by making it mean zero and unit standard deviation\n",
    "\t\"\"\"\n",
    "# \tfor i in range(StdDev.size):\n",
    "\t\t# if (StdDev[i] < 0.02):\n",
    "# \t\t\tStdDev[i] = 0.02\n",
    "\t\n",
    "\tInputs = np.divide((Inputs-Mean),StdDev)\n",
    "\treturn Inputs\n",
    "\t\n",
    "\t\n",
    "class Model:\n",
    "\n",
    "\tdef __init__(self,neurons,activation,optimizer,errFunc = 'mse',epochs=200, wd= 0.01,dropout = 0.0):\n",
    "\t\tself.neurons = neurons;\n",
    "\t\tself.errFunc = errFunc;\n",
    "\t\tself.activation = activation;\n",
    "\t\tself.optimizer = optimizer;\n",
    "\t\tself.epochs = epochs;\n",
    "\t\tself.wd = wd;\n",
    "\t\tself.dropout = dropout;\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef modelTrainValidate(self,data):\n",
    "\t\t\n",
    "\t\t# Callback used to track crossentropy loss during training\n",
    "\t\n",
    "\t\tclass LossHistory(keras.callbacks.Callback):\n",
    "\t\t\tdef on_train_begin(self,logs={}):\n",
    "\t\t\t\tself.losses = []\n",
    "\t\t\n",
    "\t\t\tdef on_batch_end(self, batch, logs={}):\n",
    "\t\t\t\tself.losses.append(logs.get('loss'))\n",
    "\t\t\n",
    "\t\t# Build the NN model\n",
    "\t\t\n",
    "\t\tmodel = Sequential();\n",
    "\t\tmodel.add(Dense(self.neurons,input_dim = data.features,init = 'glorot_uniform', W_regularizer = l2(self.wd),activation=self.activation[0]));\n",
    "\t\tmodel.add(Dropout(self.dropout));\n",
    "\t\tmodel.add(Dense(1,activation = self.activation[1]));\n",
    "\t\tmodel.compile(loss = self.errFunc, optimizer = self.optimizer);\n",
    "\t\t\n",
    "\t\thistory = LossHistory()\n",
    "\t\t\n",
    "\t\t# Train the model\n",
    "\t\t\n",
    "\t\tmodel.fit(data.trainInputs, data.trainTargets, nb_epoch = self.epochs, batch_size = 32, callbacks = [history]);\n",
    "\t\tpredictions = model.predict(data.trainInputs);\n",
    "\t\t\n",
    "\t\tprint '*****PREDICTIONS*****'\n",
    "\t\t\n",
    "\t\tprint predictions\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(history.losses)\n",
    "\t\tplt.show()\n",
    "\t\t\n",
    "\t\t\n",
    "def main():\n",
    "\t\n",
    "\tactivation = np.array(['relu','relu'],dtype=object)\n",
    "\t\n",
    "\terror_function = 'mse'\n",
    "\toptimizer_model = 'sgd'\n",
    "\t\n",
    "\tdata1 = Data('./train.csv','./train_labels.txt',0.30);\n",
    "\t\n",
    "\tdata1.inputPreprocess();\n",
    "\t\n",
    "\tmodel1 = Model(150,activation,optimizer_model);\n",
    "\t\n",
    "\tmodel1.modelTrainValidate(data1);\n",
    "\t\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE.PRM.TENR             8.462768e+01\n",
      "SE.PRM.TENR.FE          8.157562e+01\n",
      "SE.PRM.TENR.MA          8.540648e+01\n",
      "NY.ADJ.NNTY.KD.ZG       3.851237e+00\n",
      "NY.ADJ.NNTY.KD          1.621273e+12\n",
      "NY.ADJ.NNTY.CD          9.929433e+11\n",
      "NY.ADJ.NNTY.PC.KD.ZG    2.163827e+00\n",
      "NY.ADJ.NNTY.PC.KD       7.589344e+03\n",
      "NY.ADJ.NNTY.PC.CD       5.100576e+03\n",
      "NY.ADJ.DCO2.GN.ZS       5.851388e-01\n",
      "NY.ADJ.DCO2.CD          7.534420e+08\n",
      "NY.ADJ.DKAP.GN.ZS       1.032384e+01\n",
      "NY.ADJ.DKAP.CD          2.605851e+10\n",
      "NY.ADJ.AEDU.GN.ZS       3.979355e+00\n",
      "NY.ADJ.AEDU.CD          7.909355e+09\n",
      "NY.ADJ.DNGY.GN.ZS       3.739394e+00\n",
      "NY.ADJ.DNGY.CD          2.847644e+09\n",
      "NY.ADJ.DMIN.GN.ZS       5.832514e-01\n",
      "NY.ADJ.DMIN.CD          3.627539e+08\n",
      "NY.ADJ.DRES.GN.ZS       6.098861e+00\n",
      "NY.ADJ.DFOR.GN.ZS       1.423187e+00\n",
      "NY.ADJ.DFOR.CD          2.363728e+08\n",
      "SP.ADO.TFRT             7.711841e+01\n",
      "SP.POP.DPND             7.262675e+01\n",
      "SP.POP.DPND.OL          1.003267e+01\n",
      "SP.POP.DPND.YG          6.250853e+01\n",
      "AG.LND.AGRI.ZS          3.764228e+01\n",
      "AG.LND.AGRI.K2          1.498215e+06\n",
      "AG.AGR.TRAC.NO          7.847197e+05\n",
      "AG.LND.TRAC.ZS          2.718881e+02\n",
      "                            ...     \n",
      "AG.SRF.TOTL.K2          4.088581e+06\n",
      "SE.PRM.PRS5.ZS          7.167168e+01\n",
      "SE.PRM.PRSL.ZS          7.082391e+01\n",
      "SP.DYN.TO65.FE.ZS       6.722988e+01\n",
      "SP.DYN.TO65.MA.ZS       5.808639e+01\n",
      "SE.PRM.TCHR             9.867937e+05\n",
      "SE.SEC.TCHR             1.210901e+06\n",
      "SE.SEC.TCHR.FE          6.707097e+05\n",
      "BX.GRT.TECH.CD.WD       1.212852e+08\n",
      "NY.TTF.GNFS.KN         -5.185258e+11\n",
      "ER.PTD.TOTL.ZS          9.990606e+00\n",
      "ER.LND.PTLD.ZS          1.153325e+01\n",
      "SE.PRM.DURS             5.624766e+00\n",
      "SE.SEC.DURS             6.396601e+00\n",
      "NY.GDP.TOTL.RT.ZS       1.015427e+01\n",
      "NE.TRD.GNFS.ZS          7.318153e+01\n",
      "IP.TMK.NRES             2.428399e+04\n",
      "IP.TMK.RESD             6.690406e+04\n",
      "IP.TMK.TOTL             8.780589e+04\n",
      "SH.TBS.DTEC.ZS          6.583029e+01\n",
      "SL.UEM.TOTL.FE.ZS       1.055618e+01\n",
      "SL.UEM.TOTL.MA.ZS       7.965421e+00\n",
      "SL.UEM.TOTL.ZS          8.770697e+00\n",
      "SL.UEM.1524.FE.ZS       2.029703e+01\n",
      "SL.UEM.1524.MA.ZS       1.624209e+01\n",
      "SL.UEM.1524.ZS          1.750959e+01\n",
      "SP.URB.TOTL             6.747273e+07\n",
      "SP.URB.TOTL.IN.ZS       4.736675e+01\n",
      "SP.URB.GROW             3.062454e+00\n",
      "SH.DYN.AIDS.FE.ZS       4.319245e+01\n",
      "dtype: float64\n",
      "447\n",
      "[[ 93.88252     94.33384     93.45259    ...,  73.827        0.8701368\n",
      "   14.58531935]\n",
      " [ 93.85820007  91.42984772  96.25016785 ...,  46.756        3.27697215\n",
      "   31.01722674]\n",
      " [ 57.28087     51.998       62.53891    ...,  25.78         8.53059783\n",
      "   43.19244673]\n",
      " ..., \n",
      " [ 98.94835     81.57561603  85.40648061 ...,  78.905        1.24615224\n",
      "   43.19244673]\n",
      " [ 84.62768034  81.57561603  85.40648061 ...,  17.63442279   3.37565741\n",
      "   43.19244673]\n",
      " [ 90.5446      91.00683     90.09313    ...,  31.5          7.41468614\n",
      "   43.19244673]]\n",
      "[[  8.57969940e-01   1.10100430e+00   8.86720834e-01 ...,   1.08270186e+00\n",
      "   -6.84977105e-01  -3.85807375e+00]\n",
      " [  8.55715361e-01   8.50396693e-01   1.19502767e+00 ...,  -2.49908246e-02\n",
      "    6.70248532e-02  -1.64199977e+00]\n",
      " [ -2.53518614e+00  -2.55247772e+00  -2.52011877e+00 ...,  -8.83287963e-01\n",
      "    1.70849019e+00   1.91653377e-15]\n",
      " ..., \n",
      " [  1.32759773e+00   2.45272574e-15   4.69831828e-15 ...,   1.29048375e+00\n",
      "   -5.67493229e-01   1.91653377e-15]\n",
      " [  1.31741733e-15   2.45272574e-15   4.69831828e-15 ...,  -1.21658915e+00\n",
      "    9.78584977e-02   1.91653377e-15]\n",
      " [  5.48528056e-01   8.13891273e-01   5.16491817e-01 ...,  -6.49236684e-01\n",
      "    1.35982996e+00   1.91653377e-15]]\n",
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 0s - loss: 4150.9515     \n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 0s - loss: 4092.1829     \n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 0s - loss: 3533.6134     \n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 0s - loss: 2292.2604     \n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 0s - loss: 1859.5608     \n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 0s - loss: 1558.5589     \n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 0s - loss: 1985.8947     \n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 0s - loss: 2736.2041     \n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 0s - loss: 2807.7273     \n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 1s - loss: 2567.0453     \n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 0s - loss: 3166.5702     \n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 0s - loss: 3373.7793     \n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 0s - loss: 3178.8972     \n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 0s - loss: 3056.1493     \n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 0s - loss: 3413.8551     \n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 0s - loss: 3711.8917     \n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 0s - loss: 3464.8688     \n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 0s - loss: 3290.7704     \n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 2s - loss: 3253.7814     \n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 0s - loss: 3244.3682     \n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 0s - loss: 3138.9391     \n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 0s - loss: 3007.8760     \n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 0s - loss: 1125.0662     \n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 0s - loss: 1008.5157     \n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 0s - loss: 920.8923     \n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 0s - loss: 870.5355     \n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 0s - loss: 1258.2338     \n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 0s - loss: 1804.7056     \n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 0s - loss: 1584.7996     \n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 0s - loss: 1244.5106     \n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 0s - loss: 1010.9589     \n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 0s - loss: 1022.8702     \n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 0s - loss: 975.7775     \n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 0s - loss: 833.6687     \n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 0s - loss: 908.0654     \n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 0s - loss: 942.8695     \n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 0s - loss: 1053.1270     \n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 0s - loss: 1336.6752     \n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 0s - loss: 1153.4353     \n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 0s - loss: 1277.2102     \n",
      "Epoch 195/200\n",
      " 512/8000 [>.............................] - ETA: 0s - loss: 1206.0697"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tmain()\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
